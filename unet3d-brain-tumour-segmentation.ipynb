{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:47.951147Z",
     "iopub.status.busy": "2023-06-02T10:56:47.950462Z",
     "iopub.status.idle": "2023-06-02T10:56:55.220823Z",
     "shell.execute_reply": "2023-06-02T10:56:55.219875Z",
     "shell.execute_reply.started": "2023-06-02T10:56:47.951108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from random import randint\n",
    " \n",
    "import gc \n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import nibabel as nib\n",
    "import pydicom as pdm\n",
    "import nilearn as nl\n",
    "import nilearn.plotting as nlplt\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as anim\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from skimage.util import montage\n",
    "\n",
    "from IPython.display import Image as show_gif\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import MSELoss\n",
    "import albumentations as A\n",
    "from albumentations import Compose, HorizontalFlip\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:55.223152Z",
     "iopub.status.busy": "2023-06-02T10:56:55.222787Z",
     "iopub.status.idle": "2023-06-02T10:56:55.229278Z",
     "shell.execute_reply": "2023-06-02T10:56:55.228091Z",
     "shell.execute_reply.started": "2023-06-02T10:56:55.223121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# code wall time starts(cw0)\n",
    "cw0 = time.time() \n",
    "\n",
    "# data loading time starts(dl0)\n",
    "dl0 = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:55.232583Z",
     "iopub.status.busy": "2023-06-02T10:56:55.231552Z",
     "iopub.status.idle": "2023-06-02T10:56:55.27206Z",
     "shell.execute_reply": "2023-06-02T10:56:55.271203Z",
     "shell.execute_reply.started": "2023-06-02T10:56:55.232516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_filename = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_flair.nii'\n",
    "sample_filename_mask = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_seg.nii'\n",
    "\n",
    "sample_img = nib.load(sample_filename)\n",
    "sample_img = np.asanyarray(sample_img.dataobj)\n",
    "sample_img = np.rot90(sample_img)\n",
    "sample_mask = nib.load(sample_filename_mask)\n",
    "sample_mask = np.asanyarray(sample_mask.dataobj)\n",
    "sample_mask = np.rot90(sample_mask)\n",
    "print(\"img shape ->\", sample_img.shape)\n",
    "print(\"mask shape ->\", sample_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:55.274961Z",
     "iopub.status.busy": "2023-06-02T10:56:55.274623Z",
     "iopub.status.idle": "2023-06-02T10:56:55.454473Z",
     "shell.execute_reply": "2023-06-02T10:56:55.453545Z",
     "shell.execute_reply.started": "2023-06-02T10:56:55.274929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_filename2 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t1.nii'\n",
    "sample_img2 = nib.load(sample_filename2)\n",
    "sample_img2 = np.asanyarray(sample_img2.dataobj)\n",
    "sample_img2  = np.rot90(sample_img2)\n",
    "\n",
    "sample_filename3 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t2.nii'\n",
    "sample_img3 = nib.load(sample_filename3)\n",
    "sample_img3 = np.asanyarray(sample_img3.dataobj)\n",
    "sample_img3  = np.rot90(sample_img3)\n",
    "\n",
    "sample_filename4 = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_001/BraTS20_Training_001_t1ce.nii'\n",
    "sample_img4 = nib.load(sample_filename4)\n",
    "sample_img4 = np.asanyarray(sample_img4.dataobj)\n",
    "sample_img4  = np.rot90(sample_img4)\n",
    "\n",
    "# WHOLE TUOMUR / ED - LABEL 2 \n",
    "mask_WT = sample_mask.copy()\n",
    "mask_WT[mask_WT == 1] = 1\n",
    "mask_WT[mask_WT == 2] = 1\n",
    "mask_WT[mask_WT == 4] = 1\n",
    "\n",
    "# NCR OR NET - LABEL 1 \n",
    "mask_TC = sample_mask.copy()\n",
    "mask_TC[mask_TC == 1] = 1\n",
    "mask_TC[mask_TC == 2] = 0\n",
    "mask_TC[mask_TC == 4] = 1\n",
    "\n",
    "# ET - LABEL 4 \n",
    "mask_ET = sample_mask.copy()\n",
    "mask_ET[mask_ET == 1] = 0\n",
    "mask_ET[mask_ET == 2] = 0\n",
    "mask_ET[mask_ET == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:55.456994Z",
     "iopub.status.busy": "2023-06-02T10:56:55.456269Z",
     "iopub.status.idle": "2023-06-02T10:56:55.463502Z",
     "shell.execute_reply": "2023-06-02T10:56:55.462341Z",
     "shell.execute_reply.started": "2023-06-02T10:56:55.45696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total data loading time(dlt) \n",
    "dl1 = time.time() \n",
    "dlt = dl1 - dl0\n",
    "print(\"Data loading time : \", dlt )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:55.465554Z",
     "iopub.status.busy": "2023-06-02T10:56:55.465253Z",
     "iopub.status.idle": "2023-06-02T10:56:58.13934Z",
     "shell.execute_reply": "2023-06-02T10:56:58.138479Z",
     "shell.execute_reply.started": "2023-06-02T10:56:55.46551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "gs = gridspec.GridSpec(nrows=2, ncols=4, height_ratios=[1, 1.5])\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "flair = ax0.imshow(sample_img[:,:,65], cmap='bone')\n",
    "ax0.set_title(\"FLAIR\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(flair)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "t1 = ax1.imshow(sample_img2[:,:,65], cmap='bone')\n",
    "ax1.set_title(\"T1\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t1)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "t2 = ax2.imshow(sample_img3[:,:,65], cmap='bone')\n",
    "ax2.set_title(\"T2\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t2)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax3 = fig.add_subplot(gs[0, 3])\n",
    "t1ce = ax3.imshow(sample_img4[:,:,65], cmap='bone')\n",
    "ax3.set_title(\"T1 contrast\", fontsize=18, weight='bold', y=-0.2)\n",
    "fig.colorbar(t1ce)\n",
    "\n",
    "#  Varying density along a streamline\n",
    "ax4 = fig.add_subplot(gs[1, 1:3])\n",
    "\n",
    "#ax4.imshow(np.ma.masked_where(mask_WT[:,:,65]== False,  mask_WT[:,:,65]), cmap='summer', alpha=0.6)\n",
    "l1 = ax4.imshow(mask_WT[:,:,65], cmap='summer',)\n",
    "l2 = ax4.imshow(np.ma.masked_where(mask_TC[:,:,65]== False,  mask_TC[:,:,65]), cmap='rainbow', alpha=0.6)\n",
    "l3 = ax4.imshow(np.ma.masked_where(mask_ET[:,:,65] == False, mask_ET[:,:,65]), cmap='winter', alpha=0.6)\n",
    "\n",
    "ax4.set_title(\"\", fontsize=20, weight='bold', y=-0.1)\n",
    "\n",
    "_ = [ax.set_axis_off() for ax in [ax0,ax1,ax2,ax3, ax4]]\n",
    "\n",
    "colors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]]\n",
    "labels = ['Non-Enhancing tumor core', 'Peritumoral Edema ', 'GD-enhancing tumor']\n",
    "patches = [ mpatches.Patch(color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n",
    "# put those patched as legend-handles into the legend\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 'xx-large',\n",
    "           title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n",
    "\n",
    "plt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\", fontsize=20, weight='bold')\n",
    "\n",
    "fig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "fig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image conversions - 2D , 3D & GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.141565Z",
     "iopub.status.busy": "2023-06-02T10:56:58.140905Z",
     "iopub.status.idle": "2023-06-02T10:56:58.178839Z",
     "shell.execute_reply": "2023-06-02T10:56:58.17796Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.141514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Image3dToGIF3d:\n",
    "    \"\"\"\n",
    "    Displaying 3D images in 3d axes.\n",
    "    Parameters:\n",
    "        img_dim: shape of cube for resizing.\n",
    "        figsize: figure size for plotting in inches.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 img_dim: tuple = (55, 55, 55),\n",
    "                 figsize: tuple = (15, 10),\n",
    "                 binary: bool = False,\n",
    "                 normalizing: bool = True,\n",
    "                ):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.img_dim = img_dim\n",
    "        print(img_dim)\n",
    "        self.figsize = figsize\n",
    "        self.binary = binary\n",
    "        self.normalizing = normalizing\n",
    "\n",
    "    def _explode(self, data: np.ndarray):\n",
    "        \"\"\"\n",
    "        Takes: array and return an array twice as large in each dimension,\n",
    "        with an extra space between each voxel.\n",
    "        \"\"\"\n",
    "        shape_arr = np.array(data.shape)\n",
    "        size = shape_arr[:3] * 2 - 1\n",
    "        exploded = np.zeros(np.concatenate([size, shape_arr[3:]]),\n",
    "                            dtype=data.dtype)\n",
    "        exploded[::2, ::2, ::2] = data\n",
    "        return exploded\n",
    "\n",
    "    def _expand_coordinates(self, indices: np.ndarray):\n",
    "        x, y, z = indices\n",
    "        x[1::2, :, :] += 1\n",
    "        y[:, 1::2, :] += 1\n",
    "        z[:, :, 1::2] += 1\n",
    "        return x, y, z\n",
    "    \n",
    "    def _normalize(self, arr: np.ndarray):\n",
    "        \"\"\"Normilize image value between 0 and 1.\"\"\"\n",
    "        arr_min = np.min(arr)\n",
    "        return (arr - arr_min) / (np.max(arr) - arr_min)\n",
    "\n",
    "    \n",
    "    def _scale_by(self, arr: np.ndarray, factor: int):\n",
    "        \"\"\"\n",
    "        Scale 3d Image to factor.\n",
    "        Parameters:\n",
    "            arr: 3d image for scalling.\n",
    "            factor: factor for scalling.\n",
    "        \"\"\"\n",
    "        mean = np.mean(arr)\n",
    "        return (arr - mean) * factor + mean\n",
    "    \n",
    "    def get_transformed_data(self, data: np.ndarray):\n",
    "        \"\"\"Data transformation: normalization, scaling, resizing.\"\"\"\n",
    "        if self.binary:\n",
    "            resized_data = resize(data, self.img_dim, preserve_range=True)\n",
    "            return np.clip(resized_data.astype(np.uint8), 0, 1).astype(np.float32)\n",
    "            \n",
    "        norm_data = np.clip(self._normalize(data)-0.1, 0, 1) ** 0.4\n",
    "        scaled_data = np.clip(self._scale_by(norm_data, 2) - 0.1, 0, 1)\n",
    "        resized_data = resize(scaled_data, self.img_dim, preserve_range=True)\n",
    "        \n",
    "        return resized_data\n",
    "    \n",
    "    def plot_cube(self,\n",
    "                  cube,\n",
    "                  title: str = '', \n",
    "                  init_angle: int = 0,\n",
    "                  make_gif: bool = False,\n",
    "                  path_to_save: str = 'filename.gif'\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Plot 3d data.\n",
    "        Parameters:\n",
    "            cube: 3d data\n",
    "            title: title for figure.\n",
    "            init_angle: angle for image plot (from 0-360).\n",
    "            make_gif: if True create gif from every 5th frames from 3d image plot.\n",
    "            path_to_save: path to save GIF file.\n",
    "            \"\"\"\n",
    "        if self.binary:\n",
    "            facecolors = cm.winter(cube)\n",
    "            print(\"binary\")\n",
    "        else:\n",
    "            if self.normalizing:\n",
    "                cube = self._normalize(cube)\n",
    "            facecolors = cm.gist_stern(cube)\n",
    "            print(\"not binary\")\n",
    "            \n",
    "        facecolors[:,:,:,-1] = cube\n",
    "        facecolors = self._explode(facecolors)\n",
    "\n",
    "        filled = facecolors[:,:,:,-1] != 0\n",
    "        x, y, z = self._expand_coordinates(np.indices(np.array(filled.shape) + 1))\n",
    "\n",
    "        with plt.style.context(\"dark_background\"):\n",
    "\n",
    "            fig = plt.figure(figsize=self.figsize)\n",
    "            ax = fig.gca(projection='3d')\n",
    "\n",
    "            ax.view_init(30, init_angle)\n",
    "            ax.set_xlim(right = self.img_dim[0] * 2)\n",
    "            ax.set_ylim(top = self.img_dim[1] * 2)\n",
    "            ax.set_zlim(top = self.img_dim[2] * 2)\n",
    "            ax.set_title(title, fontsize=18, y=1.05)\n",
    "\n",
    "            ax.voxels(x, y, z, filled, facecolors=facecolors, shade=False)\n",
    "\n",
    "            if make_gif:\n",
    "                images = []\n",
    "                for angle in tqdm(range(0, 360, 5)):\n",
    "                    ax.view_init(30, angle)\n",
    "                    fname = str(angle) + '.png'\n",
    "\n",
    "                    plt.savefig(fname, dpi=120, format='png', bbox_inches='tight')\n",
    "                    images.append(imageio.imread(fname))\n",
    "                    #os.remove(fname)\n",
    "                imageio.mimsave(path_to_save, images)\n",
    "                plt.close()\n",
    "\n",
    "            else:\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "class ShowResult:\n",
    "  \n",
    "    def mask_preprocessing(self, mask):\n",
    "        \"\"\"\n",
    "        Test.\n",
    "        \"\"\"\n",
    "        mask = mask.squeeze().cpu().detach().numpy()\n",
    "        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "\n",
    "        mask_WT = np.rot90(montage(mask[0]))\n",
    "        mask_TC = np.rot90(montage(mask[1]))\n",
    "        mask_ET = np.rot90(montage(mask[2]))\n",
    "\n",
    "        return mask_WT, mask_TC, mask_ET\n",
    "\n",
    "    def image_preprocessing(self, image):\n",
    "        \"\"\"\n",
    "        Returns image flair as mask for overlaping gt and predictions.\n",
    "        \"\"\"\n",
    "        image = image.squeeze().cpu().detach().numpy()\n",
    "        image = np.moveaxis(image, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        flair_img = np.rot90(montage(image[0]))\n",
    "        return flair_img\n",
    "    \n",
    "    def plot(self, image, ground_truth, prediction):\n",
    "        image = self.image_preprocessing(image)\n",
    "        gt_mask_WT, gt_mask_TC, gt_mask_ET = self.mask_preprocessing(ground_truth)\n",
    "        pr_mask_WT, pr_mask_TC, pr_mask_ET = self.mask_preprocessing(prediction)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize = (35, 30))\n",
    "    \n",
    "        [ax.axis(\"off\") for ax in axes]\n",
    "        axes[0].set_title(\"Ground Truth\", fontsize=35, weight='bold')\n",
    "        axes[0].imshow(image, cmap ='bone')\n",
    "        axes[0].imshow(np.ma.masked_where(gt_mask_WT == False, gt_mask_WT),\n",
    "                  cmap='cool_r', alpha=0.6)\n",
    "        axes[0].imshow(np.ma.masked_where(gt_mask_TC == False, gt_mask_TC),\n",
    "                  cmap='autumn_r', alpha=0.6)\n",
    "        axes[0].imshow(np.ma.masked_where(gt_mask_ET == False, gt_mask_ET),\n",
    "                  cmap='autumn', alpha=0.6)\n",
    "\n",
    "        axes[1].set_title(\"Prediction\", fontsize=35, weight='bold')\n",
    "        axes[1].imshow(image, cmap ='bone')\n",
    "        axes[1].imshow(np.ma.masked_where(pr_mask_WT == False, pr_mask_WT),\n",
    "                  cmap='cool_r', alpha=0.6)\n",
    "        axes[1].imshow(np.ma.masked_where(pr_mask_TC == False, pr_mask_TC),\n",
    "                  cmap='autumn_r', alpha=0.6)\n",
    "        axes[1].imshow(np.ma.masked_where(pr_mask_ET == False, pr_mask_ET),\n",
    "                  cmap='autumn', alpha=0.6)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def merging_two_gif(path1: str, path2: str, name_to_save: str):\n",
    "    \"\"\"\n",
    "    Merging GIFs side by side.\n",
    "    Parameters:\n",
    "        path1: path to gif with ground truth.\n",
    "        path2: path to gif with prediction.\n",
    "        name_to_save: name for saving new GIF.\n",
    "    \"\"\"\n",
    "    gif1 = imageio.get_reader(path1)\n",
    "    gif2 = imageio.get_reader(path2)\n",
    "\n",
    "    number_of_frames = min(gif1.get_length(), gif2.get_length()) \n",
    "\n",
    "    new_gif = imageio.get_writer(name_to_save)\n",
    "\n",
    "    for frame_number in range(number_of_frames):\n",
    "        img1 = gif1.get_next_data()\n",
    "        img2 = gif2.get_next_data()\n",
    "        #here is the magic\n",
    "        new_image = np.hstack((img1, img2))\n",
    "        new_gif.append_data(new_image)\n",
    "\n",
    "    gif1.close()\n",
    "    gif2.close()    \n",
    "    new_gif.close()\n",
    "\n",
    "def get_all_csv_file(root: str) -> list:\n",
    "    \"\"\"Extraction all unique ids from file names.\"\"\"\n",
    "    ids = []\n",
    "    for dirname, _, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirname, filename)\n",
    "            if path.endswith(\".csv\"):\n",
    "                ids.append(path) \n",
    "    ids = list(set(filter(None, ids)))\n",
    "    print(f\"Extracted {len(ids)} csv files.\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.180989Z",
     "iopub.status.busy": "2023-06-02T10:56:58.180201Z",
     "iopub.status.idle": "2023-06-02T10:56:58.222022Z",
     "shell.execute_reply": "2023-06-02T10:56:58.221211Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.180958Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GlobalConfig:\n",
    "    root_dir = '../input/brats20-dataset-training-validation'\n",
    "    train_root_dir = '../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData'\n",
    "    test_root_dir = '../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData'\n",
    "    path_to_csv = './train_data.csv'\n",
    "    pretrained_model_path = '../input/brats20logs/brats2020logs/unet/last_epoch_model.pth'\n",
    "    train_logs_path = '../input/brats20logs/brats2020logs/unet/train_log.csv'\n",
    "    ae_pretrained_model_path = '../input/brats20logs/brats2020logs/ae/autoencoder_best_model.pth'\n",
    "    tab_data = '../input/brats20logs/brats2020logs/data/df_with_voxel_stats_and_latent_features.csv'\n",
    "    seed = 55\n",
    "    \n",
    "def seed_everything(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    \n",
    "config = GlobalConfig()\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.224912Z",
     "iopub.status.busy": "2023-06-02T10:56:58.224275Z",
     "iopub.status.idle": "2023-06-02T10:56:58.22911Z",
     "shell.execute_reply": "2023-06-02T10:56:58.228295Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.224877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data pre-processing time starts(dpp0)\n",
    "dpp0 = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.234279Z",
     "iopub.status.busy": "2023-06-02T10:56:58.233465Z",
     "iopub.status.idle": "2023-06-02T10:56:58.272655Z",
     "shell.execute_reply": "2023-06-02T10:56:58.271854Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.234247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "survival_info_df = pd.read_csv('../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/survival_info.csv')\n",
    "name_mapping_df = pd.read_csv('../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/name_mapping.csv')\n",
    "\n",
    "name_mapping_df.rename({'BraTS_2020_subject_ID': 'Brats20ID'}, axis=1, inplace=True) \n",
    "\n",
    "df = survival_info_df.merge(name_mapping_df, on=\"Brats20ID\", how=\"right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.274161Z",
     "iopub.status.busy": "2023-06-02T10:56:58.273868Z",
     "iopub.status.idle": "2023-06-02T10:56:58.303717Z",
     "shell.execute_reply": "2023-06-02T10:56:58.302838Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.274138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "for _, row  in df.iterrows():\n",
    "    \n",
    "    id_ = row['Brats20ID']\n",
    "    phase = id_.split(\"_\")[-2]\n",
    "    if phase == 'Training':\n",
    "        path = os.path.join(config.train_root_dir, id_)\n",
    "    else:\n",
    "        path = os.path.join(config.test_root_dir, id_)\n",
    "    paths.append(path)\n",
    "    \n",
    "df['path'] = paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.305717Z",
     "iopub.status.busy": "2023-06-02T10:56:58.30533Z",
     "iopub.status.idle": "2023-06-02T10:56:58.317376Z",
     "shell.execute_reply": "2023-06-02T10:56:58.316328Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.305685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning - removing all null age entries\n",
    "train_data = df.loc[df['Age'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# Calculating Age rank for the basis of K - Fold stratification\n",
    "train_data[\"Age_rank\"] =  train_data[\"Age\"] // 10 * 10\n",
    "train_data = train_data.loc[train_data['Brats20ID'] != 'BraTS20_Training_355'].reset_index(drop=True, )\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.319517Z",
     "iopub.status.busy": "2023-06-02T10:56:58.319187Z",
     "iopub.status.idle": "2023-06-02T10:56:58.323435Z",
     "shell.execute_reply": "2023-06-02T10:56:58.322598Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.319487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# stratified k-fold ( skf ) time starts \n",
    "skf0 = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.325618Z",
     "iopub.status.busy": "2023-06-02T10:56:58.324953Z",
     "iopub.status.idle": "2023-06-02T10:56:58.342798Z",
     "shell.execute_reply": "2023-06-02T10:56:58.341962Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.325588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(\n",
    "    n_splits=7, random_state=config.seed, shuffle=True\n",
    ") \n",
    "for i, (train_index, val_index) in enumerate(\n",
    "        skf.split(train_data, train_data[\"Age_rank\"])\n",
    "        ):\n",
    "        train_data.loc[val_index, \"fold\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.345874Z",
     "iopub.status.busy": "2023-06-02T10:56:58.345628Z",
     "iopub.status.idle": "2023-06-02T10:56:58.352771Z",
     "shell.execute_reply": "2023-06-02T10:56:58.351695Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.345852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total stratification time(skft) \n",
    "skf1 = time.time()\n",
    "skft = skf1 - skf0 \n",
    "print(\"Stratification time : \",skft ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.356158Z",
     "iopub.status.busy": "2023-06-02T10:56:58.354306Z",
     "iopub.status.idle": "2023-06-02T10:56:58.366414Z",
     "shell.execute_reply": "2023-06-02T10:56:58.36528Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.356131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = train_data.loc[train_data['fold'] != 0].reset_index(drop=True)\n",
    "val_df = train_data.loc[train_data['fold'] == 0].reset_index(drop=True)\n",
    "test_df = df.loc[~df['Age'].notnull()].reset_index(drop=True)\n",
    "print(\"train_df ->\", train_df.shape, \"val_df ->\", val_df.shape, \"test_df ->\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.369372Z",
     "iopub.status.busy": "2023-06-02T10:56:58.368496Z",
     "iopub.status.idle": "2023-06-02T10:56:58.374509Z",
     "shell.execute_reply": "2023-06-02T10:56:58.373593Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.369348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dpp1 = time.time() \n",
    "dppt = dpp1 - dpp0 - skft\n",
    "print(\"Data preprocessing time : \", dppt  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.376759Z",
     "iopub.status.busy": "2023-06-02T10:56:58.376028Z",
     "iopub.status.idle": "2023-06-02T10:56:58.396775Z",
     "shell.execute_reply": "2023-06-02T10:56:58.395967Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.376729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_df.csv\", index=False)\n",
    "train_df.to_csv(\"train_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.398953Z",
     "iopub.status.busy": "2023-06-02T10:56:58.398209Z",
     "iopub.status.idle": "2023-06-02T10:56:58.417846Z",
     "shell.execute_reply": "2023-06-02T10:56:58.416995Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.39892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BratsDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, phase: str=\"test\", is_resize: bool=False):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "        self.augmentations = get_augmentations(phase)\n",
    "        self.data_types = ['_flair.nii', '_t1.nii', '_t1ce.nii', '_t2.nii']\n",
    "        self.is_resize = is_resize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # at a specified index ( idx ) select the value under 'Brats20ID' & asssign it to id_ \n",
    "        id_ = self.df.loc[idx, 'Brats20ID']\n",
    "        \n",
    "        # As we've got the id_ , now find the path of the entry by asserting the Brats20ID to id_ \n",
    "        root_path = self.df.loc[self.df['Brats20ID'] == id_]['path'].values[0]\n",
    "        \n",
    "        # load all modalities\n",
    "        images = []\n",
    "        \n",
    "        for data_type in self.data_types:\n",
    "            # here data_type is appended to the root path, as it only contains the name without the datatype such as .nii etc\n",
    "            img_path = os.path.join(root_path, id_ + data_type) \n",
    "            img = self.load_img(img_path)#.transpose(2, 0, 1)\n",
    "            \n",
    "            if self.is_resize:\n",
    "                img = self.resize(img)\n",
    "    \n",
    "            img = self.normalize(img)\n",
    "            images.append(img)\n",
    "            \n",
    "        # stacking all the t1 , t1ce , t2 , t2 flair files of a single ID in a stack \n",
    "        img = np.stack(images)\n",
    "        img = np.moveaxis(img, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "        \n",
    "        if self.phase != \"test\":\n",
    "            mask_path =  os.path.join(root_path, id_ + \"_seg.nii\")\n",
    "            mask = self.load_img(mask_path)\n",
    "            \n",
    "            if self.is_resize:\n",
    "                mask = self.resize(mask)\n",
    "                # mask --> conversion to uint8 --> normalization / clipping ( 0 to 1 ) --> conversion to float32 \n",
    "                mask = np.clip(mask.astype(np.uint8), 0, 1).astype(np.float32)\n",
    "                # again clipping ( 0 to 1 ) \n",
    "                mask = np.clip(mask, 0, 1)\n",
    "            \n",
    "            # setting the mask labels 1 , 2 , 4 for the mask file ( _seg.ii ) \n",
    "            mask = self.preprocess_mask_labels(mask)\n",
    "    \n",
    "            augmented = self.augmentations(image=img.astype(np.float32), \n",
    "                                           mask=mask.astype(np.float32))\n",
    "            # Several augmentations / transformations like flipping, rotating, padding will be applied to both the images \n",
    "            img = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "    \n",
    "        \n",
    "            return {\n",
    "                \"Id\": id_,\n",
    "                \"image\": img,\n",
    "                \"mask\": mask,\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            \"Id\": id_,\n",
    "            \"image\": img,\n",
    "        }\n",
    "    \n",
    "    def load_img(self, file_path):\n",
    "        data = nib.load(file_path)\n",
    "        data = np.asarray(data.dataobj)\n",
    "        return data\n",
    "    \n",
    "    def normalize(self, data: np.ndarray):\n",
    "        data_min = np.min(data)\n",
    "        # normalization = (each element - min element) / ( max - min ) \n",
    "        return (data - data_min) / (np.max(data) - data_min)\n",
    "    \n",
    "    def resize(self, data: np.ndarray):\n",
    "        data = resize(data, (78, 120, 120), preserve_range=True)\n",
    "        return data\n",
    "    \n",
    "    def preprocess_mask_labels(self, mask: np.ndarray):\n",
    "\n",
    "        # whole tumour\n",
    "        mask_WT = mask.copy()\n",
    "        mask_WT[mask_WT == 1] = 1\n",
    "        mask_WT[mask_WT == 2] = 1\n",
    "        mask_WT[mask_WT == 4] = 1\n",
    "        # include all tumours \n",
    "\n",
    "        # NCR / NET - LABEL 1\n",
    "        mask_TC = mask.copy()\n",
    "        mask_TC[mask_TC == 1] = 1\n",
    "        mask_TC[mask_TC == 2] = 0\n",
    "        mask_TC[mask_TC == 4] = 1\n",
    "        # exclude 2 / 4 labelled tumour \n",
    "        \n",
    "        # ET - LABEL 4 \n",
    "        mask_ET = mask.copy()\n",
    "        mask_ET[mask_ET == 1] = 0\n",
    "        mask_ET[mask_ET == 2] = 0\n",
    "        mask_ET[mask_ET == 4] = 1\n",
    "        # exclude 2 / 1 labelled tumour \n",
    "        \n",
    "        # ED - LABEL 2\n",
    "        # mask_ED = mask.copy()\n",
    "        # mask_ED[mask_ED == 1] = 0\n",
    "        # mask_ED[mask_ED == 2] = 1\n",
    "        # mask_ED[mask_ED == 4] = 0\n",
    "\n",
    "\n",
    "        # mask = np.stack([mask_WT, mask_TC, mask_ET, mask_ED])\n",
    "        mask = np.stack([mask_WT, mask_TC, mask_ET])\n",
    "        mask = np.moveaxis(mask, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "\n",
    "        return mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.41966Z",
     "iopub.status.busy": "2023-06-02T10:56:58.4193Z",
     "iopub.status.idle": "2023-06-02T10:56:58.43169Z",
     "shell.execute_reply": "2023-06-02T10:56:58.430806Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.41963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_augmentations(phase):\n",
    "    list_transforms = []\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    path_to_csv: str,\n",
    "    phase: str,\n",
    "    fold: int = 0,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4 ):\n",
    "    \n",
    "    '''Returns: dataloader for the model training'''\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    train_df = df.loc[df['fold'] != fold].reset_index(drop=True)\n",
    "    val_df = df.loc[df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    if phase == \"train\" : \n",
    "        df = train_df \n",
    "    elif phase == \"valid\" :\n",
    "        df = val_df\n",
    "#     else:\n",
    "#         df = test_df\n",
    "    dataset = dataset(df, phase)\n",
    "    \"\"\"\n",
    "    DataLoader iteratively goes through every id in the df & gets all the individual tuples for individual ids & appends all of them \n",
    "    like this : \n",
    "    { id : ['BraTS20_Training_235'] ,\n",
    "      image : [] , \n",
    "      tensor : [] , \n",
    "    } \n",
    "    { id : ['BraTS20_Training_236'] ,\n",
    "      image : [] , \n",
    "      tensor : [] , \n",
    "    } \n",
    "    { id : ['BraTS20_Training_237'] ,\n",
    "      image : [] , \n",
    "      tensor : [] , \n",
    "    } \n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.433627Z",
     "iopub.status.busy": "2023-06-02T10:56:58.433007Z",
     "iopub.status.idle": "2023-06-02T10:56:58.451525Z",
     "shell.execute_reply": "2023-06-02T10:56:58.450455Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.433597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase='valid', fold=0)\n",
    "len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:56:58.453577Z",
     "iopub.status.busy": "2023-06-02T10:56:58.452965Z",
     "iopub.status.idle": "2023-06-02T10:57:08.861709Z",
     "shell.execute_reply": "2023-06-02T10:57:08.860564Z",
     "shell.execute_reply.started": "2023-06-02T10:56:58.453545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(dataloader))\n",
    "data['Id'], data['image'].shape, data['mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:08.864841Z",
     "iopub.status.busy": "2023-06-02T10:57:08.863351Z",
     "iopub.status.idle": "2023-06-02T10:57:08.870923Z",
     "shell.execute_reply": "2023-06-02T10:57:08.869862Z",
     "shell.execute_reply.started": "2023-06-02T10:57:08.86479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(data['image'].shape)\n",
    "print(data['mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:08.873066Z",
     "iopub.status.busy": "2023-06-02T10:57:08.872471Z",
     "iopub.status.idle": "2023-06-02T10:57:13.095644Z",
     "shell.execute_reply": "2023-06-02T10:57:13.094838Z",
     "shell.execute_reply.started": "2023-06-02T10:57:08.873034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Printing all the images & masks of \"FOLD 0\" --> 34 batches \n",
    "# 34 BATCHES of images & masks each batch containing 4 types of files t1 , t2 , t1ce , flair \n",
    "\n",
    "img_tensor = data['image'].squeeze()[0].cpu().detach().numpy() \n",
    "mask_tensor = data['mask'].squeeze()[0].squeeze().cpu().detach().numpy()\n",
    "\n",
    "print(\"Num uniq Image values :\", len(np.unique(img_tensor, return_counts=True)[0]))\n",
    "print(\"Min/Max Image values:\", img_tensor.min(), img_tensor.max())\n",
    "print(\"Num uniq Mask values:\", np.unique(mask_tensor, return_counts=True))\n",
    "\n",
    "image = np.rot90(montage(img_tensor))\n",
    "mask = np.rot90(montage(mask_tensor)) \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (20, 20))\n",
    "ax.imshow(image, cmap ='bone')\n",
    "ax.imshow(np.ma.masked_where(mask == False, mask),\n",
    "           cmap='cool', alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.097824Z",
     "iopub.status.busy": "2023-06-02T10:57:13.096842Z",
     "iopub.status.idle": "2023-06-02T10:57:13.146504Z",
     "shell.execute_reply": "2023-06-02T10:57:13.145572Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.097791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img_tensor.shape \n",
    "mask_tensor.shape\n",
    "\n",
    "image = np.rot90(montage(img_tensor))\n",
    "mask = np.rot90(montage(mask_tensor)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.148668Z",
     "iopub.status.busy": "2023-06-02T10:57:13.148286Z",
     "iopub.status.idle": "2023-06-02T10:57:13.180939Z",
     "shell.execute_reply": "2023-06-02T10:57:13.180034Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.148634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def dice_coef_metric(probabilities: torch.Tensor,\n",
    "                     truth: torch.Tensor, \n",
    "                     treshold: float = 0.5,\n",
    "                     eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Dice score for data batch.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: truth values.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        Returns: dice score aka f1.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    num = probabilities.shape[0] \n",
    "    predictions = (probabilities >= treshold).float()\n",
    "    assert(predictions.shape == truth.shape)\n",
    "    for i in range(num):\n",
    "        prediction = predictions[i]\n",
    "        truth_ = truth[i]\n",
    "        intersection = 2.0 * (truth_ * prediction).sum()\n",
    "        union = truth_.sum() + prediction.sum()\n",
    "        if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append((intersection + eps) / union)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def jaccard_coef_metric(probabilities: torch.Tensor,\n",
    "               truth: torch.Tensor,\n",
    "               treshold: float = 0.5,\n",
    "               eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Jaccard index for data batch.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: truth values.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        Returns: jaccard score aka iou.\"\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    num = probabilities.shape[0]\n",
    "    predictions = (probabilities >= treshold).float()\n",
    "    assert(predictions.shape == truth.shape)\n",
    "\n",
    "    for i in range(num):\n",
    "        prediction = predictions[i]\n",
    "        truth_ = truth[i]\n",
    "        intersection = (prediction * truth_).sum()\n",
    "        union = (prediction.sum() + truth_.sum()) - intersection + eps\n",
    "        if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append((intersection + eps) / union)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "class Meter:\n",
    "    '''factory for storing and updating iou and dice scores.'''\n",
    "    def __init__(self, treshold: float = 0.5):\n",
    "        self.threshold: float = treshold\n",
    "        self.dice_scores: list = []\n",
    "        self.iou_scores: list = []\n",
    "    \n",
    "    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes: logits from output model and targets,\n",
    "        calculates dice and iou scores, and stores them in lists.\n",
    "        calculates using the above declare functions \n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dice = dice_coef_metric(probs, targets, self.threshold)\n",
    "        iou = jaccard_coef_metric(probs, targets, self.threshold)\n",
    "        \n",
    "        # appending to the respective lists \n",
    "        self.dice_scores.append(dice)\n",
    "        self.iou_scores.append(iou)\n",
    "    \n",
    "    def get_metrics(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns: the average of the accumulated dice and iou scores.\n",
    "        \"\"\"\n",
    "        dice = np.mean(self.dice_scores)\n",
    "        iou = np.mean(self.iou_scores)\n",
    "        return dice, iou\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Calculate dice loss.\"\"\"\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,\n",
    "                logits: torch.Tensor,\n",
    "                targets: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        num = targets.size(0)\n",
    "        probability = torch.sigmoid(logits)\n",
    "        probability = probability.view(num, -1)\n",
    "        targets = targets.view(num, -1)\n",
    "        assert(probability.shape == targets.shape)\n",
    "        \n",
    "        intersection = 2.0 * (probability * targets).sum()\n",
    "        union = probability.sum() + targets.sum()\n",
    "        dice_score = (intersection + self.eps) / union\n",
    "        #print(\"intersection\", intersection, union, dice_score)\n",
    "        return 1.0 - dice_score\n",
    "        \n",
    "        \n",
    "class BCEDiceLoss(nn.Module):\n",
    "    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, \n",
    "                logits: torch.Tensor,\n",
    "                targets: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # logits are the images \n",
    "        # target are the masks \n",
    "        assert(logits.shape == targets.shape)\n",
    "        dice_loss = self.dice(logits, targets)\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        \n",
    "        # binary cross entropy loss & dice loss \n",
    "        return bce_loss + dice_loss\n",
    "    \n",
    "# helper functions for testing.  \n",
    "def dice_coef_metric_per_classes(probabilities: np.ndarray,\n",
    "                                    truth: np.ndarray,\n",
    "                                    treshold: float = 0.5,\n",
    "                                    eps: float = 1e-9,\n",
    "                                    classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Dice score for data batch and for each class i.e. 'WT', 'TC', 'ET'\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: model targets.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        classes: list with name classes.\n",
    "        Returns: dict with dice scores for each class.\n",
    "    \"\"\"\n",
    "    scores = {key: list() for key in classes}\n",
    "    num = probabilities.shape[0]\n",
    "    num_classes = probabilities.shape[1]\n",
    "    predictions = (probabilities >= treshold).astype(np.float32)\n",
    "    assert(predictions.shape == truth.shape)\n",
    "\n",
    "    for i in range(num):\n",
    "        for class_ in range(num_classes):\n",
    "            prediction = predictions[i][class_]\n",
    "            truth_ = truth[i][class_]\n",
    "            intersection = 2.0 * (truth_ * prediction).sum()\n",
    "            union = truth_.sum() + prediction.sum()\n",
    "            if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "                 scores[classes[class_]].append(1.0)\n",
    "            else:\n",
    "                scores[classes[class_]].append((intersection + eps) / union)\n",
    "                \n",
    "    return scores\n",
    "\n",
    "\n",
    "def jaccard_coef_metric_per_classes(probabilities: np.ndarray, # output of the model in an array format \n",
    "               truth: np.ndarray,# masks  \n",
    "               treshold: float = 0.5, # threshold to whether segment / not \n",
    "               eps: float = 1e-9, # smooth \n",
    "               classes: list = ['WT', 'TC', 'ET']) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Jaccard index for data batch and for each class.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: model targets.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        classes: list with name classes.\n",
    "        Returns: dict with jaccard scores for each class.\"\n",
    "    \"\"\"\n",
    "    scores = {key: list() for key in classes}\n",
    "    # storing all the jaccard coefficients in a list \n",
    "    \n",
    "    num = probabilities.shape[0]\n",
    "    \n",
    "    num_classes = probabilities.shape[1]\n",
    "    \n",
    "    # segmenting if prob > threshold .i.e. setting to float32 \n",
    "    predictions = (probabilities >= treshold).astype(np.float32)\n",
    "    \n",
    "    assert(predictions.shape == truth.shape)\n",
    "\n",
    "    for i in range(num):\n",
    "        for class_ in range(num_classes):\n",
    "            prediction = predictions[i][class_]\n",
    "            truth_ = truth[i][class_]\n",
    "            intersection = (prediction * truth_).sum()\n",
    "            union = (prediction.sum() + truth_.sum()) - intersection + eps\n",
    "            if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "                 scores[classes[class_]].append(1.0)\n",
    "            else:\n",
    "                scores[classes[class_]].append((intersection + eps) / union)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.183146Z",
     "iopub.status.busy": "2023-06-02T10:57:13.182598Z",
     "iopub.status.idle": "2023-06-02T10:57:13.205098Z",
     "shell.execute_reply": "2023-06-02T10:57:13.204225Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.183113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_groups=8):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # Convlution set one \n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.BatchNorm3d(out_channels),\n",
    "            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Convlution set two \n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            #nn.BatchNorm3d(out_channels),\n",
    "            nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "            \n",
    "          )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "    \n",
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.MaxPool3d(2, 2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # max pooling 3d + doubleConv \n",
    "        return self.encoder(x)\n",
    "\n",
    "    \n",
    "class Up(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, trilinear=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if trilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "            \n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2, diffZ // 2, diffZ - diffZ // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "class Out(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet3d(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes, n_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        # extracting the features by incrementally multiplying the no.of channels \n",
    "        self.conv = DoubleConv(in_channels, n_channels)\n",
    "        self.enc1 = Down(n_channels, 2 * n_channels)\n",
    "        self.enc2 = Down(2 * n_channels, 4 * n_channels)\n",
    "        self.enc3 = Down(4 * n_channels, 8 * n_channels)\n",
    "        self.enc4 = Down(8 * n_channels, 8 * n_channels)\n",
    "\n",
    "        self.dec1 = Up(16 * n_channels, 4 * n_channels)\n",
    "        self.dec2 = Up(8 * n_channels, 2 * n_channels)\n",
    "        self.dec3 = Up(4 * n_channels, n_channels)\n",
    "        self.dec4 = Up(2 * n_channels, n_channels)\n",
    "        self.out = Out(n_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv(x)\n",
    "        x2 = self.enc1(x1)\n",
    "        x3 = self.enc2(x2)\n",
    "        x4 = self.enc3(x3)\n",
    "        x5 = self.enc4(x4)\n",
    "\n",
    "        mask = self.dec1(x5, x4)\n",
    "        mask = self.dec2(mask, x3)\n",
    "        mask = self.dec3(mask, x2)\n",
    "        mask = self.dec4(mask, x1)\n",
    "        mask = self.out(mask)\n",
    "        \n",
    "        \"\"\"\n",
    "        After a series of either Upsampling / 3d Transpose\n",
    "        a segmented image of the input image is generated \n",
    "        & returned \n",
    "        \"\"\"\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.216562Z",
     "iopub.status.busy": "2023-06-02T10:57:13.215898Z",
     "iopub.status.idle": "2023-06-02T10:57:13.245527Z",
     "shell.execute_reply": "2023-06-02T10:57:13.244615Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.216513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 net: nn.Module,\n",
    "                 dataset: torch.utils.data.Dataset,\n",
    "                 criterion: nn.Module,\n",
    "                 lr: float,\n",
    "                 accumulation_steps: int,\n",
    "                 batch_size: int,\n",
    "                 fold: int,\n",
    "                 num_epochs: int,\n",
    "                 path_to_csv: str,\n",
    "                 display_plot: bool = True,\n",
    "                ):\n",
    "\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"device:\", self.device)\n",
    "        self.display_plot = display_plot\n",
    "        self.net = net\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = Adam(self.net.parameters(), lr=lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\",\n",
    "                                           patience=2, verbose=True)\n",
    "        self.accumulation_steps = accumulation_steps // batch_size\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.dataloaders = {\n",
    "            phase: get_dataloader(\n",
    "                dataset = dataset,\n",
    "                path_to_csv = path_to_csv,\n",
    "                phase = phase,\n",
    "                fold = fold,\n",
    "                batch_size = batch_size,\n",
    "                num_workers = 4\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.best_loss = float(\"inf\")\n",
    "        \n",
    "        # calculating the list of losses for both train & validation phases \n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        \n",
    "        # calculating the dice scores for both train & validation phases \n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "        # calculating the jaccard scores for both train & validation phases\n",
    "        self.jaccard_scores = {phase: [] for phase in self.phases}\n",
    "         \n",
    "    def _compute_loss_and_outputs(self,\n",
    "                                  images: torch.Tensor,\n",
    "                                  targets: torch.Tensor):\n",
    "        images = images.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        logits = self.net(images)\n",
    "        loss = self.criterion(logits, targets)\n",
    "        return loss, logits\n",
    "        \n",
    "    def _do_epoch(self, epoch: int, phase: str):\n",
    "        print(f\"{phase} epoch: {epoch} | time: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "        self.net.train() if phase == \"train\" else self.net.eval()\n",
    "        meter = Meter()\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        total_batches = len(dataloader)\n",
    "        running_loss = 0.0 \n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, data_batch in enumerate(dataloader):\n",
    "            images, targets = data_batch['image'], data_batch['mask']\n",
    "            # BCEDiceLoss & raw prediction( logits ) are calculated \n",
    "            loss, logits = self._compute_loss_and_outputs(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                # Backpropagating the losses generated to train the Unet \n",
    "                loss.backward()\n",
    "                if (itr + 1) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            print(f\"running loss of epoch {epoch} is : \", running_loss) \n",
    "            # meter.update stores running_loss for each iteration in one epoch in a list to visualize in graph \n",
    "            meter.update(logits.detach().cpu(),\n",
    "                         targets.detach().cpu()\n",
    "                        )\n",
    "            \n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        epoch_dice, epoch_iou = meter.get_metrics()\n",
    "        \n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(epoch_dice)\n",
    "        self.jaccard_scores[phase].append(epoch_iou)\n",
    "\n",
    "        return epoch_loss\n",
    "        \n",
    "    def run(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self._do_epoch(epoch, \"train\")\n",
    "            with torch.no_grad():\n",
    "                val_loss = self._do_epoch(epoch, \"val\")\n",
    "                print(f\"BCEDiceLoss for epoch {epoch} is : \" , val_loss ) \n",
    "                self.scheduler.step(val_loss)\n",
    "            if self.display_plot:\n",
    "                self._plot_train_history()\n",
    "                \n",
    "            if val_loss < self.best_loss:\n",
    "                print(f\"\\n{'#'*20}\\nSaved new checkpoint\\n{'#'*20}\\n\")\n",
    "                self.best_loss = val_loss\n",
    "                torch.save(self.net.state_dict(), \"best_model.pth\")\n",
    "            print()\n",
    "        self._save_train_history()\n",
    "            \n",
    "    def _plot_train_history(self):\n",
    "        data = [self.losses, self.dice_scores, self.jaccard_scores]\n",
    "        colors = ['deepskyblue', \"crimson\"]\n",
    "        labels = [\n",
    "            f\"\"\"\n",
    "            train loss {self.losses['train'][-1]}\n",
    "            val loss {self.losses['val'][-1]}\n",
    "            \"\"\",\n",
    "            \n",
    "            f\"\"\"\n",
    "            train dice score {self.dice_scores['train'][-1]}\n",
    "            val dice score {self.dice_scores['val'][-1]} \n",
    "            \"\"\", \n",
    "                  \n",
    "            f\"\"\"\n",
    "            train jaccard score {self.jaccard_scores['train'][-1]}\n",
    "            val jaccard score {self.jaccard_scores['val'][-1]}\n",
    "            \"\"\",\n",
    "        ]\n",
    "        \n",
    "        clear_output(True)\n",
    "        with plt.style.context(\"seaborn-dark-palette\"):\n",
    "            fig, axes = plt.subplots(3, 1, figsize=(8, 10))\n",
    "            for i, ax in enumerate(axes):\n",
    "                ax.plot(data[i]['val'], c=colors[0], label=\"val\")\n",
    "                ax.plot(data[i]['train'], c=colors[-1], label=\"train\")\n",
    "                ax.set_title(labels[i])\n",
    "                ax.legend(loc=\"upper right\")\n",
    "                \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    def load_predtrain_model(self,\n",
    "                             state_path: str):\n",
    "        self.net.load_state_dict(torch.load(state_path))\n",
    "        print(\"Predtrain model loaded\")\n",
    "        \n",
    "    def _save_train_history(self):\n",
    "        \"\"\"writing model weights and training logs to files.\"\"\"\n",
    "        torch.save(self.net.state_dict(),\n",
    "                   f\"last_epoch_model.pth\")\n",
    "\n",
    "        logs_ = [self.losses, self.dice_scores, self.jaccard_scores]\n",
    "        log_names_ = [\"_loss\", \"_dice\", \"_jaccard\"]\n",
    "        logs = [logs_[i][key] for i in list(range(len(logs_)))\n",
    "                         for key in logs_[i]]\n",
    "        log_names = [key+log_names_[i] \n",
    "                     for i in list(range(len(logs_))) \n",
    "                     for key in logs_[i]\n",
    "                    ]\n",
    "        pd.DataFrame(\n",
    "            dict(zip(log_names, logs))\n",
    "        ).to_csv(\"train_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.248267Z",
     "iopub.status.busy": "2023-06-02T10:57:13.247346Z",
     "iopub.status.idle": "2023-06-02T10:57:13.315Z",
     "shell.execute_reply": "2023-06-02T10:57:13.314154Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.248235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nodel = UNet3d(in_channels=4, n_classes=3, n_channels=24).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.316522Z",
     "iopub.status.busy": "2023-06-02T10:57:13.316185Z",
     "iopub.status.idle": "2023-06-02T10:57:13.700156Z",
     "shell.execute_reply": "2023-06-02T10:57:13.69917Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.31649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(net=nodel,\n",
    "                  dataset=BratsDataset,\n",
    "                  criterion=BCEDiceLoss(),\n",
    "                  lr=5e-4,\n",
    "                  accumulation_steps=4,\n",
    "                  batch_size=1,\n",
    "                  fold=0,\n",
    "                  num_epochs=1,\n",
    "                  path_to_csv = config.path_to_csv,)\n",
    "\n",
    "if config.pretrained_model_path is not None:\n",
    "    trainer.load_predtrain_model(config.pretrained_model_path)\n",
    "    \n",
    "    # if need - load the logs.      \n",
    "    train_logs = pd.read_csv(config.train_logs_path)\n",
    "    trainer.losses[\"train\"] =  train_logs.loc[:, \"train_loss\"].to_list()\n",
    "    trainer.losses[\"val\"] =  train_logs.loc[:, \"val_loss\"].to_list()\n",
    "    trainer.dice_scores[\"train\"] = train_logs.loc[:, \"train_dice\"].to_list()\n",
    "    trainer.dice_scores[\"val\"] = train_logs.loc[:, \"val_dice\"].to_list()\n",
    "    trainer.jaccard_scores[\"train\"] = train_logs.loc[:, \"train_jaccard\"].to_list()\n",
    "    trainer.jaccard_scores[\"val\"] = train_logs.loc[:, \"val_jaccard\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.702167Z",
     "iopub.status.busy": "2023-06-02T10:57:13.701513Z",
     "iopub.status.idle": "2023-06-02T10:57:13.707006Z",
     "shell.execute_reply": "2023-06-02T10:57:13.705779Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.702132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T10:57:13.709299Z",
     "iopub.status.busy": "2023-06-02T10:57:13.708311Z",
     "iopub.status.idle": "2023-06-02T11:16:46.601836Z",
     "shell.execute_reply": "2023-06-02T11:16:46.600595Z",
     "shell.execute_reply.started": "2023-06-02T10:57:13.709266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:46.604124Z",
     "iopub.status.busy": "2023-06-02T11:16:46.603757Z",
     "iopub.status.idle": "2023-06-02T11:16:46.609618Z",
     "shell.execute_reply": "2023-06-02T11:16:46.608605Z",
     "shell.execute_reply.started": "2023-06-02T11:16:46.604085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total training time(tt) \n",
    "t1 = time.time()\n",
    "tt = t1 - t0 \n",
    "print(\"Training time : \",tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:46.612164Z",
     "iopub.status.busy": "2023-06-02T11:16:46.611422Z",
     "iopub.status.idle": "2023-06-02T11:16:46.712456Z",
     "shell.execute_reply": "2023-06-02T11:16:46.711516Z",
     "shell.execute_reply.started": "2023-06-02T11:16:46.612132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(nodel.state_dict(), 'unet3d_state_dict.pth')\n",
    "torch.save(nodel, 'unet3d_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:46.714081Z",
     "iopub.status.busy": "2023-06-02T11:16:46.713735Z",
     "iopub.status.idle": "2023-06-02T11:16:46.967071Z",
     "shell.execute_reply": "2023-06-02T11:16:46.965957Z",
     "shell.execute_reply.started": "2023-06-02T11:16:46.714052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:46.969696Z",
     "iopub.status.busy": "2023-06-02T11:16:46.968906Z",
     "iopub.status.idle": "2023-06-02T11:16:47.541622Z",
     "shell.execute_reply": "2023-06-02T11:16:47.540764Z",
     "shell.execute_reply.started": "2023-06-02T11:16:46.969661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nodel = torch.load('/kaggle/input/brats-ser-models-and-dataframes/unet3d_model.pth')\n",
    "nodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:47.543742Z",
     "iopub.status.busy": "2023-06-02T11:16:47.543389Z",
     "iopub.status.idle": "2023-06-02T11:16:47.559813Z",
     "shell.execute_reply": "2023-06-02T11:16:47.558828Z",
     "shell.execute_reply.started": "2023-06-02T11:16:47.543711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataloader = get_dataloader(dataset=BratsDataset, path_to_csv='train_data.csv', phase=\"valid\", fold=1)\n",
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:16:47.561661Z",
     "iopub.status.busy": "2023-06-02T11:16:47.561301Z",
     "iopub.status.idle": "2023-06-02T11:17:50.451087Z",
     "shell.execute_reply": "2023-06-02T11:17:50.449858Z",
     "shell.execute_reply.started": "2023-06-02T11:16:47.56163Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "gc.collect() \n",
    "def compute_metrics(model, dataloader, threshold=0.33):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    counter = 0  # Counter to keep track of the number of entries processed\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations to save memory\n",
    "        for data in dataloader:\n",
    "            \n",
    "            images, targets = data['image'], data['mask']\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = (probabilities >= threshold).float()\n",
    "\n",
    "            # Compute binary segmentation metrics\n",
    "            true_positives += torch.sum((predictions == 1) & (targets == 1)).item()\n",
    "            false_positives += torch.sum((predictions == 1) & (targets == 0)).item()\n",
    "            true_negatives += torch.sum((predictions == 0) & (targets == 0)).item()\n",
    "            false_negatives += torch.sum((predictions == 0) & (targets == 1)).item()\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # Free memory by clearing intermediate variables\n",
    "            del images, targets, logits, probabilities, predictions\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return true_positives , false_positives , true_negatives , false_negatives\n",
    "\n",
    "tp , fp , tn , fn  = compute_metrics(nodel, test_dataloader, threshold=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.453243Z",
     "iopub.status.busy": "2023-06-02T11:17:50.452853Z",
     "iopub.status.idle": "2023-06-02T11:17:50.460603Z",
     "shell.execute_reply": "2023-06-02T11:17:50.459276Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.453202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"True positives : {tp}\")\n",
    "print(f\"False positives : {fp}\")\n",
    "print(f\"True Negatives : {tn}\")\n",
    "print(f\"False Negatives : {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.462764Z",
     "iopub.status.busy": "2023-06-02T11:17:50.461881Z",
     "iopub.status.idle": "2023-06-02T11:17:50.737465Z",
     "shell.execute_reply": "2023-06-02T11:17:50.736546Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.462733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(tp, fp, tn, fn):\n",
    "    confusion_matrix = np.array([[tn, fp], [fn, tp]])\n",
    "    labels = ['True ', 'False ']\n",
    "    cmap = plt.cm.Blues\n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    thresh = confusion_matrix.max() / 2.\n",
    "    for i, j in np.ndindex(confusion_matrix.shape):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], 'd'), horizontalalignment='center', color='white' if confusion_matrix[i, j] > thresh else 'black')\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(tp, fp, tn, fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.7408Z",
     "iopub.status.busy": "2023-06-02T11:17:50.738835Z",
     "iopub.status.idle": "2023-06-02T11:17:50.746357Z",
     "shell.execute_reply": "2023-06-02T11:17:50.745377Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.740765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.748424Z",
     "iopub.status.busy": "2023-06-02T11:17:50.747603Z",
     "iopub.status.idle": "2023-06-02T11:17:50.759339Z",
     "shell.execute_reply": "2023-06-02T11:17:50.758413Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.748391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Accuracy : {accuracy*100}\")\n",
    "print(f\"Precision : {precision*100}\")\n",
    "print(f\"Recall : {recall*100}\")\n",
    "print(f\"F1 Score : {f1_score*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.761385Z",
     "iopub.status.busy": "2023-06-02T11:17:50.760952Z",
     "iopub.status.idle": "2023-06-02T11:17:50.769676Z",
     "shell.execute_reply": "2023-06-02T11:17:50.768779Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.761252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def compute_metrics(model, dataloader, num_entries, threshold=0.33):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    predictions = []\n",
    "    counter = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            if counter >= num_entries:\n",
    "                break \n",
    "\n",
    "            images, targets = data['image'], data['mask']\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            prediction = (probabilities >= threshold).float()\n",
    "\n",
    "            prediction =  prediction.cpu()\n",
    "            targets = targets.cpu()\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "\n",
    "            model.zero_grad()\n",
    "            del images, targets, logits, probabilities, prediction\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            counter += 1\n",
    "    y_true = np.concatenate([targets.cpu() for data in dataloader for targets in data['mask']])\n",
    "    y_pred = np.concatenate([prediction.cpu() for data in dataloader for prediction in predictions])\n",
    "    \n",
    "    # y_true = np.concatenate([targets for targets in dataloader.dataset])\n",
    "    # y_pred = np.concatenate([predictions for predictions in predictions])\n",
    "\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Compute classification report\n",
    "    class_names = ['Background', 'Tumor']\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "\n",
    "    # Return evaluation metrics, confusion matrix, and classification report\n",
    "    evaluation_results = {\n",
    "        'Confusion Matrix': cm,\n",
    "        'Classification Report': report\n",
    "    }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "num_entries = 5 \n",
    "evaluation_results = compute_metrics(nodel, test_dataloader, num_entries, threshold=0.33)\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(f'{metric}: {value}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.771487Z",
     "iopub.status.busy": "2023-06-02T11:17:50.771065Z",
     "iopub.status.idle": "2023-06-02T11:17:50.783313Z",
     "shell.execute_reply": "2023-06-02T11:17:50.78266Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.771456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_scores_per_classes(model,          # nodel which is UNeT3D \n",
    "                               dataloader,     # tuple consisting of ( id , image tensor , mask tensor )\n",
    "                               classes):       # classes : WT , TC , ET \n",
    "    \"\"\"\n",
    "    Compute Dice and Jaccard coefficients for each class.\n",
    "    Params:\n",
    "        model: neural net for make predictions.\n",
    "        dataloader: dataset object to load data from.\n",
    "        classes: list with classes.\n",
    "        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dice_scores_per_classes = {key: list() for key in classes}\n",
    "    iou_scores_per_classes = {key: list() for key in classes}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            imgs, targets = data['image'], data['mask']\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            logits = model(imgs)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            # Now finding the overlap between the raw prediction i.e. logit & the mask i.e. target & finding the dice & iou scores \n",
    "            dice_scores = dice_coef_metric_per_classes(logits, targets)\n",
    "            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n",
    "\n",
    "            # storing both dice & iou scores in the list declared \n",
    "            for key in dice_scores.keys():\n",
    "                dice_scores_per_classes[key].extend(dice_scores[key])\n",
    "\n",
    "            for key in iou_scores.keys():\n",
    "                iou_scores_per_classes[key].extend(iou_scores[key])\n",
    "\n",
    "    return dice_scores_per_classes, iou_scores_per_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.785101Z",
     "iopub.status.busy": "2023-06-02T11:17:50.784454Z",
     "iopub.status.idle": "2023-06-02T11:17:50.812207Z",
     "shell.execute_reply": "2023-06-02T11:17:50.811304Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.785068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_dataloader = get_dataloader(BratsDataset, 'train_data.csv', phase='valid', fold=0)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.813819Z",
     "iopub.status.busy": "2023-06-02T11:17:50.813439Z",
     "iopub.status.idle": "2023-06-02T11:17:50.820579Z",
     "shell.execute_reply": "2023-06-02T11:17:50.819444Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.813771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "nodel.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:17:50.822519Z",
     "iopub.status.busy": "2023-06-02T11:17:50.822114Z",
     "iopub.status.idle": "2023-06-02T11:19:01.517016Z",
     "shell.execute_reply": "2023-06-02T11:19:01.515797Z",
     "shell.execute_reply.started": "2023-06-02T11:17:50.822488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n",
    "    nodel, val_dataloader, ['WT', 'TC', 'ET']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:01.519594Z",
     "iopub.status.busy": "2023-06-02T11:19:01.51895Z",
     "iopub.status.idle": "2023-06-02T11:19:01.545354Z",
     "shell.execute_reply": "2023-06-02T11:19:01.544282Z",
     "shell.execute_reply.started": "2023-06-02T11:19:01.519552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dice_df = pd.DataFrame(dice_scores_per_classes)\n",
    "dice_df.columns = ['WT dice', 'TC dice', 'ET dice']\n",
    "\n",
    "iou_df = pd.DataFrame(iou_scores_per_classes)\n",
    "iou_df.columns = ['WT jaccard', 'TC jaccard', 'ET jaccard']\n",
    "# CONCAT BOTH THE COLUMNS ALONG AXIS 1 & SORT THE TWO \n",
    "val_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\n",
    "val_metics_df = val_metics_df.loc[:, ['WT dice', 'WT jaccard', \n",
    "                                      'TC dice', 'TC jaccard', \n",
    "                                      'ET dice', 'ET jaccard']]\n",
    "val_metics_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:01.547635Z",
     "iopub.status.busy": "2023-06-02T11:19:01.547241Z",
     "iopub.status.idle": "2023-06-02T11:19:02.369507Z",
     "shell.execute_reply": "2023-06-02T11:19:02.368461Z",
     "shell.execute_reply.started": "2023-06-02T11:19:01.5476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "colors = ['#35FCFF', '#FF355A', '#96C503', '#C5035B', '#28B463', '#35FFAF']\n",
    "palette = sns.color_palette(colors, 6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6));\n",
    "sns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\n",
    "ax.set_xticklabels(val_metics_df.columns, fontsize=14, rotation=15);\n",
    "ax.set_title(\"Dice and Jaccard Coefficients from Validation\", fontsize=20)\n",
    "\n",
    "for idx, p in enumerate(ax.patches):\n",
    "        percentage = '{:.1f}%'.format(100 * val_metics_df.mean().values[idx])\n",
    "        x = p.get_x() + p.get_width() / 2 - 0.15\n",
    "        y = p.get_y() + p.get_height()\n",
    "        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "fig.savefig(\"result1.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n",
    "fig.savefig(\"result1.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the brain tumour segmented masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:02.371467Z",
     "iopub.status.busy": "2023-06-02T11:19:02.370846Z",
     "iopub.status.idle": "2023-06-02T11:19:02.380729Z",
     "shell.execute_reply": "2023-06-02T11:19:02.379812Z",
     "shell.execute_reply.started": "2023-06-02T11:19:02.371433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_results(model,\n",
    "                    dataloader,\n",
    "                    treshold=0.33):\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    results = {\"Id\": [],\"image\": [], \"GT\": [],\"Prediction\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            id_, imgs, targets = data['Id'], data['image'], data['mask']\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            logits = model(imgs)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            predictions = (probs >= treshold).float()\n",
    "            predictions =  predictions.cpu()\n",
    "            targets = targets.cpu()\n",
    "            \n",
    "            results[\"Id\"].append(id_)\n",
    "            results[\"image\"].append(imgs.cpu())\n",
    "            results[\"GT\"].append(targets)\n",
    "            results[\"Prediction\"].append(predictions)\n",
    "            \n",
    "            # only 5 pars\n",
    "            if (i > 5):\n",
    "                return results\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:02.382682Z",
     "iopub.status.busy": "2023-06-02T11:19:02.382264Z",
     "iopub.status.idle": "2023-06-02T11:19:02.393455Z",
     "shell.execute_reply": "2023-06-02T11:19:02.392573Z",
     "shell.execute_reply.started": "2023-06-02T11:19:02.382649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prediction time starts(p0) \n",
    "p0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:02.395449Z",
     "iopub.status.busy": "2023-06-02T11:19:02.395094Z",
     "iopub.status.idle": "2023-06-02T11:19:30.046752Z",
     "shell.execute_reply": "2023-06-02T11:19:30.045084Z",
     "shell.execute_reply.started": "2023-06-02T11:19:02.395416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = compute_results(\n",
    "    nodel, val_dataloader, 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:30.054833Z",
     "iopub.status.busy": "2023-06-02T11:19:30.054351Z",
     "iopub.status.idle": "2023-06-02T11:19:30.067825Z",
     "shell.execute_reply": "2023-06-02T11:19:30.066749Z",
     "shell.execute_reply.started": "2023-06-02T11:19:30.054784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total prediction time(pt) \n",
    "p1 = time.time()\n",
    "pt = p1 - p0 \n",
    "print(\"Model prediction time ; \",pt ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:30.069663Z",
     "iopub.status.busy": "2023-06-02T11:19:30.069104Z",
     "iopub.status.idle": "2023-06-02T11:19:30.087003Z",
     "shell.execute_reply": "2023-06-02T11:19:30.085993Z",
     "shell.execute_reply.started": "2023-06-02T11:19:30.06963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for id_, img, gt, prediction in zip(results['Id'][4:],\n",
    "                    results['image'][4:],\n",
    "                    results['GT'][4:],\n",
    "                    results['Prediction'][4:]\n",
    "                    ):\n",
    "    \n",
    "    print(id_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the predicted segmented masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:30.093047Z",
     "iopub.status.busy": "2023-06-02T11:19:30.091089Z",
     "iopub.status.idle": "2023-06-02T11:19:41.699301Z",
     "shell.execute_reply": "2023-06-02T11:19:41.698425Z",
     "shell.execute_reply.started": "2023-06-02T11:19:30.093011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_result = ShowResult()\n",
    "show_result.plot(img, gt, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D SCATTER PLOT OF THE BRAIN TUMOUR WITH : \n",
    "#### 1. necrotic and non-enhancing tumor core (NCR/NET  label 1)\n",
    "#### 2. peritumoral edema (ED  label 2)\n",
    "#### 3. GD-enhancing tumor (ET  label 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:41.701384Z",
     "iopub.status.busy": "2023-06-02T11:19:41.700847Z",
     "iopub.status.idle": "2023-06-02T11:19:44.16248Z",
     "shell.execute_reply": "2023-06-02T11:19:44.161514Z",
     "shell.execute_reply.started": "2023-06-02T11:19:41.701353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImageReader:\n",
    "    def __init__(self, root:str, img_size:int=256, normalize:bool=False, single_class:bool=False):\n",
    "        pad_size = 256 if img_size > 256 else 224\n",
    "        self.resize = A.Compose(\n",
    "            [\n",
    "                A.PadIfNeeded(min_height=pad_size, min_width=pad_size, value=0),\n",
    "                A.Resize(img_size, img_size)\n",
    "            ]\n",
    "        )\n",
    "        self.normalize=normalize\n",
    "        self.single_class=single_class\n",
    "        self.root=root\n",
    "        \n",
    "    def read_file(self, path:str) -> dict:\n",
    "        scan_type = path.split('_')[-1]\n",
    "        raw_image = nib.load(path).get_fdata()\n",
    "        raw_mask = nib.load(path.replace(scan_type, 'seg.nii')).get_fdata()\n",
    "        processed_frames, processed_masks = [], []\n",
    "        for frame_idx in range(raw_image.shape[2]):\n",
    "            frame = raw_image[:, :, frame_idx]\n",
    "            mask = raw_mask[:, :, frame_idx]\n",
    "            if self.normalize:\n",
    "                if frame.max() > 0:\n",
    "                    frame = frame/frame.max()\n",
    "                frame = frame.astype(np.float32)\n",
    "            else:\n",
    "                frame = frame.astype(np.uint8)\n",
    "            resized = self.resize(image=frame, mask=mask)\n",
    "            processed_frames.append(resized['image'])\n",
    "            processed_masks.append(1*(resized['mask'] > 0) if self.single_class else resized['mask'])\n",
    "        return {\n",
    "            'scan': np.stack(processed_frames, 0),\n",
    "            'segmentation': np.stack(processed_masks, 0),\n",
    "            'orig_shape': raw_image.shape\n",
    "        }\n",
    "    \n",
    "    def load_patient_scan(self, idx:int, scan_type:str='flair') -> dict:\n",
    "        patient_id = str(1).zfill(3) \n",
    "        scan_filename = f'{self.root}/BraTS20_Training_{patient_id}/BraTS20_Training_{patient_id}_{scan_type}.nii'\n",
    "        return self.read_file(scan_filename)\n",
    "    \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import plotly\n",
    "\n",
    "\n",
    "def generate_3d_scatter(\n",
    "    x:np.array, y:np.array, z:np.array, colors:np.array,\n",
    "    size:int=3, opacity:float=0.2, scale:str='Teal',\n",
    "    hover:str='skip', name:str='MRI'\n",
    ") -> go.Scatter3d:\n",
    "    return go.Scatter3d(\n",
    "        x=x, y=y, z=z,\n",
    "        mode='markers', hoverinfo=hover,\n",
    "        marker = dict(\n",
    "            size=size, opacity=opacity,\n",
    "            color=colors, colorscale=scale\n",
    "        ),\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageViewer3d():\n",
    "    def __init__(\n",
    "        self, reader:ImageReader, mri_downsample:int=10, mri_colorscale:str='Ice'\n",
    "    ) -> None:\n",
    "        self.reader = reader\n",
    "        self.mri_downsample = mri_downsample\n",
    "        self.mri_colorscale = mri_colorscale\n",
    "\n",
    "    def load_clean_mri(self, image:np.array, orig_dim:int) -> dict:\n",
    "        shape_offset = image.shape[1]/orig_dim\n",
    "        z, x, y = (image > 0).nonzero()\n",
    "        # only (1/mri_downsample) is sampled for the resulting image\n",
    "        x, y, z = x[::self.mri_downsample], y[::self.mri_downsample], z[::self.mri_downsample]\n",
    "        colors = image[z, x, y]\n",
    "        return dict(x=x/shape_offset, y=y/shape_offset, z=z, colors=colors)\n",
    "    def load_tumor_segmentation(self, image:np.array, orig_dim:int) -> dict:\n",
    "        tumors = {}\n",
    "        shape_offset = image.shape[1]/orig_dim\n",
    "        # 1/1, 1/3 si 1/5 pixeli pentru clasele tumorii  1(nucleu necrotic), 2(edem) si 4(tumoare de amplificare)\n",
    "        sampling = {\n",
    "            1: 1, 2: 3, 4: 5\n",
    "        }\n",
    "        for class_idx in sampling:\n",
    "            z, x, y = (image == class_idx).nonzero()\n",
    "            x, y, z = x[::sampling[class_idx]], y[::sampling[class_idx]], z[::sampling[class_idx]]\n",
    "            tumors[class_idx] = dict(\n",
    "                x=x/shape_offset, y=y/shape_offset, z=z,\n",
    "                colors=class_idx/4\n",
    "            )\n",
    "        return tumors\n",
    "    def collect_patient_data(self, scan:dict) -> tuple:\n",
    "        clean_mri = self.load_clean_mri(scan['scan'], scan['orig_shape'][0])\n",
    "        tumors = self.load_tumor_segmentation(scan['segmentation'], scan['orig_shape'][0])\n",
    "        markers_created = clean_mri['x'].shape[0] + sum(tumors[class_idx]['x'].shape[0] for class_idx in tumors)\n",
    "        return [\n",
    "            generate_3d_scatter(**clean_mri, scale=self.mri_colorscale, opacity=0.3, hover='skip', name='Brain MRI'),\n",
    "            generate_3d_scatter(**tumors[1], opacity=0.90, hover='all', name='Necrotic tumor core'),\n",
    "            generate_3d_scatter(**tumors[2], opacity=0.05, hover='all', name='Peritumoral invaded tissue'),\n",
    "            generate_3d_scatter(**tumors[4], opacity=0.30, hover='all', name='GD-enhancing tumor'),\n",
    "        ], markers_created\n",
    "    def get_3d_scan(self, patient_idx:int, scan_type:str='flair') -> go.Figure:\n",
    "        scan = self.reader.load_patient_scan(patient_idx, scan_type)\n",
    "        data, num_markers = self.collect_patient_data(scan)\n",
    "        fig = go.Figure(data=data)\n",
    "        fig.update_layout(\n",
    "            title=f\"[Patient id:{patient_idx}] brain MRI scan ({num_markers} points)\",\n",
    "            legend_title=\"Pixel class (click to enable/disable)\",\n",
    "            font=dict(\n",
    "                family=\"Courier New, monospace\",\n",
    "                size=14,\n",
    "            ),\n",
    "            margin=dict(\n",
    "                l=0,r=0,b=0,t=30\n",
    "            ),\n",
    "            legend=dict(itemsizing='constant')\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "# tumour visualization time starts(tv0)\n",
    "tv0 = time.time() \n",
    "reader = ImageReader(config.train_root_dir, img_size=128, normalize=True, single_class=False)\n",
    "viewer = ImageViewer3d(reader, mri_downsample=25)\n",
    "\n",
    "fig = viewer.get_3d_scan(250, 'flair')\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:44.164764Z",
     "iopub.status.busy": "2023-06-02T11:19:44.163865Z",
     "iopub.status.idle": "2023-06-02T11:19:44.171631Z",
     "shell.execute_reply": "2023-06-02T11:19:44.169083Z",
     "shell.execute_reply.started": "2023-06-02T11:19:44.164732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total tumour visualization time(tvt) \n",
    "tv1 = time.time() \n",
    "tvt = tv1 - tv0 \n",
    "print(\"3D Scatter plot time : \", tvt ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:19:44.173814Z",
     "iopub.status.busy": "2023-06-02T11:19:44.173099Z",
     "iopub.status.idle": "2023-06-02T11:19:44.251137Z",
     "shell.execute_reply": "2023-06-02T11:19:44.250013Z",
     "shell.execute_reply.started": "2023-06-02T11:19:44.173773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Save the scatter plot as an image file\n",
    "filename = '/kaggle/working/scatter_plot.png'  # Replace with your desired file path\n",
    "pyo.plot(fig, filename=filename, auto_open=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:20:06.65843Z",
     "iopub.status.busy": "2023-06-02T11:20:06.657524Z",
     "iopub.status.idle": "2023-06-02T11:20:06.664945Z",
     "shell.execute_reply": "2023-06-02T11:20:06.663834Z",
     "shell.execute_reply.started": "2023-06-02T11:20:06.658386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# total code wall time(cwt)\n",
    "cw1 = time.time() \n",
    "cwt = cw1 - cw0 \n",
    "print(\"Total code wall time / running time : \", cwt )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code running time evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:20:06.916348Z",
     "iopub.status.busy": "2023-06-02T11:20:06.916009Z",
     "iopub.status.idle": "2023-06-02T11:20:06.946871Z",
     "shell.execute_reply": "2023-06-02T11:20:06.945798Z",
     "shell.execute_reply.started": "2023-06-02T11:20:06.916318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Define the variables and their respective values\n",
    "variables = ['Data loading', 'data preprocessing', 'Stratification', 'Model training', 'tumour predction', 'tumour visualization', ' total code wall time']\n",
    "values = [dlt, dppt, skft, tt, pt, tvt, cwt]  # Assign the actual values to these variables\n",
    "\n",
    "# Create a list of lists containing the variables and values\n",
    "table_data = [[var, val] for var, val in zip(variables, values)]\n",
    "\n",
    "# Specify the table headers\n",
    "headers = ['Times', 'Value']\n",
    "\n",
    "# Generate the table using tabulate\n",
    "table = tabulate(table_data, headers, tablefmt='grid')\n",
    "\n",
    "# Print the table\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:20:07.047399Z",
     "iopub.status.busy": "2023-06-02T11:20:07.047079Z",
     "iopub.status.idle": "2023-06-02T11:20:07.371791Z",
     "shell.execute_reply": "2023-06-02T11:20:07.370862Z",
     "shell.execute_reply.started": "2023-06-02T11:20:07.047373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the variables and their respective values (running times)\n",
    "variables = ['Data loading', 'data preprocessing', 'Stratification', 'Model training', 'tumour predction', 'tumour visualization', ' total code wall time']\n",
    "values = [dlt, dppt, skft, tt, pt, tvt, cwt]  # Assign the actual running times to these variables\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.bar(variables, values)  # Plot the bar chart\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Variables')  # X-axis label\n",
    "plt.ylabel('Running Time')  # Y-axis label\n",
    "plt.title('Running Times of Variables')  # Chart title\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D BINARY MASK GIF PROJECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:20:07.468034Z",
     "iopub.status.busy": "2023-06-02T11:20:07.465499Z",
     "iopub.status.idle": "2023-06-02T11:20:07.473814Z",
     "shell.execute_reply": "2023-06-02T11:20:07.472594Z",
     "shell.execute_reply.started": "2023-06-02T11:20:07.467999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gt = gt.squeeze().cpu().detach().numpy()\n",
    "gt = np.moveaxis(gt, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "wt,tc, et = gt\n",
    "print(wt.shape, tc.shape, et.shape)\n",
    "gt = (wt + tc + et)\n",
    "gt = np.clip(gt, 0, 1)\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:20:07.586631Z",
     "iopub.status.busy": "2023-06-02T11:20:07.584315Z",
     "iopub.status.idle": "2023-06-02T11:20:07.592703Z",
     "shell.execute_reply": "2023-06-02T11:20:07.591701Z",
     "shell.execute_reply.started": "2023-06-02T11:20:07.586596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "title = \"Ground Truth_\" + id_[0]\n",
    "filename1 = title + \"_3d.gif\"\n",
    "\n",
    "data_to_3dgif = Image3dToGIF3d(img_dim = (120, 120, 78), binary=True, normalizing=False)\n",
    "transformed_data = data_to_3dgif.get_transformed_data(gt)\n",
    "data_to_3dgif.plot_cube(\n",
    "    transformed_data,\n",
    "    title=title,\n",
    "    make_gif=True,\n",
    "    path_to_save=filename1\n",
    ")\n",
    "#show_gif(filename1, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:21:18.135351Z",
     "iopub.status.busy": "2023-06-02T11:21:18.134616Z",
     "iopub.status.idle": "2023-06-02T11:21:18.139926Z",
     "shell.execute_reply": "2023-06-02T11:21:18.138844Z",
     "shell.execute_reply.started": "2023-06-02T11:21:18.135309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction = prediction.squeeze().cpu().detach().numpy()\n",
    "prediction = np.moveaxis(prediction, (0, 1, 2, 3), (0, 3, 2, 1))\n",
    "wt,tc,et = prediction\n",
    "print(wt.shape, tc.shape, et.shape)\n",
    "prediction = (wt + tc + et)\n",
    "prediction = np.clip(prediction, 0, 1)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:21:18.306413Z",
     "iopub.status.busy": "2023-06-02T11:21:18.30612Z",
     "iopub.status.idle": "2023-06-02T11:21:18.311168Z",
     "shell.execute_reply": "2023-06-02T11:21:18.31023Z",
     "shell.execute_reply.started": "2023-06-02T11:21:18.306387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "title = \"Prediction_\" + id_[0]\n",
    "filename2 = title + \"_3d.gif\"\n",
    "\n",
    "data_to_3dgif = Image3dToGIF3d(img_dim = (120, 120, 78), binary=True, normalizing=False)\n",
    "transformed_data = data_to_3dgif.get_transformed_data(prediction)\n",
    "data_to_3dgif.plot_cube(\n",
    "    transformed_data,\n",
    "    title=title,\n",
    "    make_gif=True,\n",
    "    path_to_save=filename2\n",
    ")\n",
    "#show_gif(filename2, format='png')#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:21:18.837273Z",
     "iopub.status.busy": "2023-06-02T11:21:18.836916Z",
     "iopub.status.idle": "2023-06-02T11:21:18.841966Z",
     "shell.execute_reply": "2023-06-02T11:21:18.840771Z",
     "shell.execute_reply.started": "2023-06-02T11:21:18.837245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "merging_two_gif(filename1,\n",
    "                filename2, \n",
    "                'result.gif')\n",
    "show_gif('result.gif', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T11:21:19.277413Z",
     "iopub.status.busy": "2023-06-02T11:21:19.276445Z",
     "iopub.status.idle": "2023-06-02T11:21:19.282603Z",
     "shell.execute_reply": "2023-06-02T11:21:19.281543Z",
     "shell.execute_reply.started": "2023-06-02T11:21:19.277381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('/kaggle/working/output_files', 'zip', '/kaggle/working')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 723383,
     "sourceId": 1267593,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 751906,
     "sourceId": 1299795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 978569,
     "sourceId": 1653290,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3345892,
     "sourceId": 5873416,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30497,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
